{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Outils sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Outils pytorch\n",
    "import torch # package Racine\n",
    "from torch import nn # Réseau de Neurones (Neural Network)\n",
    "import torch.nn.functional as F # Couches, fonctions d'activation ...\n",
    "import torch.autograd as autograd # Calcul dérivée (Gradient)\n",
    "import torch.optim as optim # Optimiser pour la descente de Gradient\n",
    "\n",
    "# Librairie graphique plotly\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cuda = torch.device('cuda:0')\n",
    "print('cuda disponible:', torch.cuda.is_available())\n",
    "\n",
    "dir(optim)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ClasseModele(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(ClasseModele, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim,50) # Première couche de 50 neurones\n",
    "        self.layer2 = nn.Linear(50, 20)       # deuxième couche de 20 neurones\n",
    "        self.layer3 = nn.Linear(20, 3)        # Couche de sortie de 3 neurones (3 classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer3(x) # Application de la fonction sofmax à la couche de sortie\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features, labels = load_iris(return_X_y=True)\n",
    "print(features.shape,labels.shape)\n",
    "iris = np.concatenate((features, labels.reshape([150,1])), axis=1)\n",
    "iris[np.random.randint(len(iris), size=10)]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features_train,features_dev, labels_train, labels_dev = train_test_split(features, labels, random_state=42)\n",
    "print(features_train.shape,features_dev.shape, labels_train.shape, labels_dev.shape)\n",
    "labels_dev[:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Création d'un objet Modèle\n",
    "model = ClasseModele(features_train.shape[1])\n",
    "model.cuda()\n",
    "\n",
    "# choix de l'algorithme de Descente de Gradient et du learning Rate\n",
    "# https://pytorch.org/docs/stable/optim.html#algorithms\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# choix de la fonction de coût\n",
    "# https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "#Nombre d'itération sur les données\n",
    "epochs = 100\n",
    "\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train, y_train = torch.from_numpy(features_train,).float().cuda(), torch.from_numpy(labels_train).long().cuda()\n",
    "for epoch in range(1, epochs+1):\n",
    "    y_pred = model(x_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    if not epoch%10 :\n",
    "        print('Epoch #%i Loss=%.4f'%(epoch,loss))\n",
    "\n",
    "    optimizer.zero_grad() # Réinitialise le gradient\n",
    "    loss.backward()       # Exécute la backpropagation\n",
    "    optimizer.step()      # Met à jours les paramètres du réseau\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Création du Tensor x_dev, de type float\n",
    "x_dev = torch.from_numpy(features_dev).float().cuda()\n",
    "#Exécution du modèle\n",
    "pred = model(x_dev)\n",
    "#Sortie pred du computation graph, et conversion en tableau numpy\n",
    "print('Avant Detach\\n','*' * 12)\n",
    "print(pred.grad_fn)\n",
    "pred = pred.detach()\n",
    "print('\\nAprès Detach:\\n','*' * 12)\n",
    "print(pred.grad_fn)\n",
    "pred = pred.cpu().numpy()\n",
    "print('\\nNumpy:\\n','*' * 12)\n",
    "pred\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prevision_type_iris =  np.argmax(model(x_dev).cpu().detach().numpy(),axis=1)\n",
    "prevision_type_iris\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print (\"Précision de la prévision = %.1f%% \"%(accuracy_score(labels_dev, prevision_type_iris)*100))\n",
    "print('%i prévision(s) correcte(s), %i erreur(s)'%(sum(prevision_type_iris==labels_dev),sum(prevision_type_iris!=labels_dev)))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "acc_train=[]\n",
    "acc_dev=[]\n",
    "tab_epoch=[]\n",
    "new_model = ClasseModele(features_train.shape[1])\n",
    "new_model.cuda()\n",
    "new_optimizer = optim.Adam(new_model.parameters(), lr=0.01)\n",
    "epocs=100\n",
    "for epoch in range(1, epochs+1):\n",
    "    y_pred = new_model(x_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    if not epoch%10 :\n",
    "        print('Epoch #%i Loss=%.2f'%(epoch,loss.item()))\n",
    "        tab_epoch.append(epoch)\n",
    "        acc_dev.append(\n",
    "            accuracy_score(\n",
    "                labels_dev,\n",
    "                np.argmax(new_model(x_dev).cpu().detach().numpy(),axis=1)\n",
    "            )*100\n",
    "        )\n",
    "        acc_train.append(\n",
    "            accuracy_score(\n",
    "                labels_train,\n",
    "                np.argmax(new_model(x_train).cpu().detach().numpy(),axis=1)\n",
    "            )*100\n",
    "        )\n",
    "    new_optimizer.zero_grad() # Réinitialise le gradient\n",
    "    loss.backward()       # Exécute la backpropagation\n",
    "    new_optimizer.step()      # Met à jours les paramètres du réseau\n",
    "\n",
    "print(acc_train,acc_dev)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_line =go.Scatter(x=tab_epoch,y=acc_train,name='Train set')\n",
    "dev_line =go.Scatter(x=tab_epoch,y=acc_dev,name='Dev set')\n",
    "\n",
    "layout = go.Layout(title=\"Comparaison Précision Train/Test\",titlefont=dict(size=40),autosize=False, width=1100,height=1100)\n",
    "\n",
    "data=[train_line,dev_line]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "fig.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
