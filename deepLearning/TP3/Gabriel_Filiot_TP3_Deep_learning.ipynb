{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-12T16:03:46.881761Z",
     "end_time": "2023-04-12T16:03:46.964542Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import csv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# 1.1 Importer les données Train et Test\n",
    "\n",
    "train_clean = pd.read_csv(\"dev/machineLearning/deepLearning/data/Module3/train_clean.csv\")\n",
    "\n",
    "test_clean = pd.read_csv(\"dev/machineLearning/deepLearning/data/Module3/test_clean.csv\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T16:03:46.939651Z",
     "end_time": "2023-04-12T16:03:46.965631Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# 1.2 Depuis le Dataframe train, charger les features d'apprentissage dans un array numpy X_alltrain, et les labels (données à prévoir) dans un array numpy y_alltrain\n",
    "\n",
    "X_alltrain = train_clean.values[:, 2:]\n",
    "y_alltrain = train_clean.values[:, 1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T16:03:46.939651Z",
     "end_time": "2023-04-12T16:03:46.965631Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions de X_train : (801, 7)\n",
      "Dimensions de y_train : (801,)\n",
      "Dimensions de X_dev : (90, 7)\n",
      "Dimensions de y_dev : (90,)\n"
     ]
    }
   ],
   "source": [
    "# 1.3 Séparer les features et les labels en deux parties (train et dev), en attribuant 10% des exemples aux données de dev. Afficher les nombres de lignes et de colonnes pour les 4 arrays.\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_alltrain, y_alltrain, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Dimensions de X_train :\", X_train.shape)\n",
    "print(\"Dimensions de y_train :\", y_train.shape)\n",
    "print(\"Dimensions de X_dev :\", X_dev.shape)\n",
    "print(\"Dimensions de y_dev :\", y_dev.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T16:03:46.939651Z",
     "end_time": "2023-04-12T16:03:46.965631Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 premières lignes de X_train :\n",
      " [[3 1 0 2 0 0 4]\n",
      " [3 0 0 3 0 0 2]\n",
      " [1 1 3 3 0 1 1]\n",
      " [3 1 2 0 2 1 1]\n",
      " [3 1 0 0 0 1 1]\n",
      " [3 1 1 0 0 1 1]\n",
      " [1 1 2 3 0 0 1]\n",
      " [3 1 2 1 0 1 1]\n",
      " [2 0 2 1 0 1 3]\n",
      " [3 1 1 0 0 1 1]]\n",
      "10 premiers éléments de y_train :\n",
      " [1 0 0 0 0 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# 1.4 Afficher les 10 premières lignes de features et les 10 premiers labels.\n",
    "\n",
    "print(\"10 premières lignes de X_train :\\n\", X_train[:10, :])\n",
    "\n",
    "print(\"10 premiers éléments de y_train :\\n\", y_train[:10])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T16:03:46.939651Z",
     "end_time": "2023-04-12T16:03:46.965631Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "# 2.1 Définir et instancier une classe Titanic Model avec les caractéristiques suivantes :\n",
    "#  - Deux couches cachées de 50 neurones.\n",
    "#  - Deux classes en sortie : Survivant ou non\n",
    "#  - Des fonctions d'activation RELU\n",
    "#  - Un dropout paramétrable pour les 2 couches cachées\n",
    "\n",
    "class TitanicModel(nn.Module):\n",
    "    def __init__(self, dropout_rate):\n",
    "        super(TitanicModel, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(7, 50)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(50, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.relu1(inputs)\n",
    "        x = self.dropout1(inputs)\n",
    "        x = self.fc2(inputs)\n",
    "        x = self.relu2(inputs)\n",
    "        x = self.dropout2(inputs)\n",
    "        x = self.fc3(inputs)\n",
    "        x = self.softmax(inputs)\n",
    "\n",
    "        output = self.output_layer(x)\n",
    "        return output\n",
    "\n",
    "titanic = TitanicModel(dropout_rate=0.4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T16:10:03.889529Z",
     "end_time": "2023-04-12T16:10:03.940904Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# 2.2 Définir des paramètres de nombre d'epochs (50) et de learning_rate (0.01)\n",
    "\n",
    "num_epochs = 50\n",
    "learning_rate = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T16:09:24.786063Z",
     "end_time": "2023-04-12T16:09:24.796212Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "# 2.3 Définir la taille du minibatch à 50. En déduire le nombre de boucles pour chaque epoch.\n",
    "\n",
    "batch_size = 50\n",
    "num_batches = len(X_train) // batch_size\n",
    "num_steps_per_epoch = num_batches + 1 if len(X_train) % batch_size != 0 else num_batches"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T16:09:25.239351Z",
     "end_time": "2023-04-12T16:09:25.410198Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "# 2.4 Définir une fonction de coût de type CrossEntropy\n",
    "\n",
    "criterion = F.cross_entropy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T16:09:25.858316Z",
     "end_time": "2023-04-12T16:09:25.863314Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "# 2.5. Définir un optimizer de type Adam, sans oublier le learning rate\n",
    "\n",
    "optimizer = optim.Adam(titanic.parameters(), learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-12T16:09:26.230555Z",
     "end_time": "2023-04-12T16:09:26.234554Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11530/1023758026.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n",
      "/tmp/ipykernel_11530/1023758026.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train, dtype=torch.long)\n",
      "/tmp/ipykernel_11530/1023758026.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_dev = torch.tensor(X_dev, dtype=torch.float32)\n",
      "/tmp/ipykernel_11530/1023758026.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_dev = torch.tensor(y_dev, dtype=torch.long)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (50x7 and 50x50)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[71], line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m X_batch \u001B[38;5;241m=\u001B[39m X_train_shuffled[i:i\u001B[38;5;241m+\u001B[39mbatch_size]\n\u001B[1;32m     18\u001B[0m y_batch \u001B[38;5;241m=\u001B[39m y_train_shuffled[i:i\u001B[38;5;241m+\u001B[39mbatch_size]\n\u001B[0;32m---> 20\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mtitanic\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, y_batch)\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Backward pass et optimisation des poids\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[66], line 24\u001B[0m, in \u001B[0;36mTitanicModel.forward\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m     22\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu1(inputs)\n\u001B[1;32m     23\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout1(inputs)\n\u001B[0;32m---> 24\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc2\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu2(inputs)\n\u001B[1;32m     26\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout2(inputs)\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (50x7 and 50x50)"
     ]
    }
   ],
   "source": [
    "# 3.1 Exécuter l'apprentissage du modèle.\n",
    "#  - Créer une boucle sur les epochs, qui contient elle-même une boucle sur les minibatchs.\n",
    "#  - À chaque nouvelle itération sur les epochs, mélanger les données avec la méthode shuffle.\n",
    "#  - Tous les 5 epochs afficher la valeur de la fonction de cout\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_dev = torch.tensor(X_dev, dtype=torch.float32)\n",
    "y_dev = torch.tensor(y_dev, dtype=torch.long)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    indices = torch.randperm(len(X_train))\n",
    "    X_train_shuffled = X_train[indices]\n",
    "    y_train_shuffled = y_train[indices]\n",
    "\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        X_batch = X_train_shuffled[i:i+batch_size]\n",
    "        y_batch = y_train_shuffled[i:i+batch_size]\n",
    "\n",
    "        outputs = titanic(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        # Backward pass et optimisation des poids\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Affichage de la perte tous les 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (90x7 and 50x50)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[73], line 8\u001B[0m\n\u001B[1;32m      6\u001B[0m dev_inputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor(X_dev)\n\u001B[1;32m      7\u001B[0m dev_labels \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor(y_dev)\n\u001B[0;32m----> 8\u001B[0m dev_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mtitanic\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdev_inputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m _, predicted \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(dev_outputs\u001B[38;5;241m.\u001B[39mdata, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     10\u001B[0m total \u001B[38;5;241m=\u001B[39m dev_labels\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[72], line 24\u001B[0m, in \u001B[0;36mTitanicModel.forward\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m     22\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu1(inputs)\n\u001B[1;32m     23\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout1(inputs)\n\u001B[0;32m---> 24\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc2\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     25\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu2(inputs)\n\u001B[1;32m     26\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout2(inputs)\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (90x7 and 50x50)"
     ]
    }
   ],
   "source": [
    "# 3.2 Calculer la précision de la prévision sur les données dev\n",
    "\n",
    "titanic.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    dev_inputs = torch.Tensor(X_dev)\n",
    "    dev_labels = torch.Tensor(y_dev)\n",
    "    dev_outputs = titanic(dev_inputs)\n",
    "    _, predicted = torch.max(dev_outputs.data, 1)\n",
    "    total = dev_labels.size(0)\n",
    "    correct = (predicted == dev_labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "\n",
    "print('Accuracy on dev set: {:.2f}%'.format(accuracy * 100))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3.3 Calculer prévisions sur les données de tests\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3.4 Générer le fichier résultat et l'envoyer sur kaggle. Quel est votre score et votre classement ?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3.5 Exécuter une cross-validation dans une boucle pour trouver les meilleures valeurs de learning rate, de keep_prob et de nombre d'epochs.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
